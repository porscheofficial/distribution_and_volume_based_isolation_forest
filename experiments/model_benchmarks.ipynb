{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarks to try and improve the run time of minlp solver scaling up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "np.random.seed(42)\n",
    "\n",
    "# For starters, we'll simply benchmark on fixed randomly generated test sets of increasing size, dimension and scale (decreased density).\n",
    "sizes = [10,100]\n",
    "dimensions = [1,2,3,10]\n",
    "scales = [1, 10]\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", \"data size dimension scale source\")\n",
    "\n",
    "datasets = {(i,j,k): Dataset(np.random.rand(i,j) * k, i, j, k, \"random\") for i,j,k in itertools.product(sizes, dimensions, scales)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some considerations on how to set the min_volume, the absolute_error and the relative_error below:\n",
    "\n",
    "As we're sampling from a uniform distribution, we know that the true value of f for every pattern is 1. In this sense, for this benchmark the ground truth is trivial. But we care about the computational effect of tweaks and settings, so this is fine (we'll also benchmark other datasets).\n",
    "\n",
    "Nevertheless, we do need to \"simulate\" an anomaly detection experiment, because we want to set ranges for the min_volume and the errors that would be relevant if\n",
    "\n",
    "The f_hat samples should then be normally distributed around 1, with decreasing variance prop to 1/sqrt(N). To remind ourselves, epsilon bounds If we want to achieve a certain delta and epsilon values, then we have to choose"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import time\n",
    "from rare_pattern_detect.minlp_based import MINLPModel\n",
    "\n",
    "Parameter = namedtuple(\"Parameter\", \"name value\")\n",
    "# solver parameters\n",
    "bound_included = [Parameter(\"bound included\", v) for v in [0.05, 1]]\n",
    "initial_patterns = [Parameter(\"initial pattern\", v) for v in [\"minimal\", \"maximal\"]]\n",
    "min_volumes = [Parameter(\"min volume\", v) for v in [0.01, 0.05, 0.1]]\n",
    "absolute_errors = [Parameter(\"absolute error\", v) for v in [1e-3, 0.1, 0.2]] # thinking this through I believe whatever absolute error we set, we can simply add it to the epsilon of the pac performance.\n",
    "relative_errors = [Parameter(\"relative error\", v) for v in [1e-3, 0.1, 0.2]] # this becomes useful especially if we set epsilon to be relative to an estimate of f.\n",
    "# use_parallel_threads = [2, 4, 6]\n",
    "# use_different_solvers = [Falase, ]\n",
    "\n",
    "parameters = [bound_included,\n",
    "              initial_patterns,\n",
    "              min_volumes,\n",
    "              absolute_errors,\n",
    "              relative_errors\n",
    "              ]\n",
    "\n",
    "def generate_solver_settings(in_dict):\n",
    "    solver_settings = {}\n",
    "    if \"absolute error\" in in_dict:\n",
    "        solver_settings[\"absolute_bound_tolerance\"] = in_dict[\"absolute error\"]\n",
    "    if \"relative error\" in in_dict:\n",
    "        solver_settings[\"relative_bound_tolerance\"] = in_dict[\"relative error\"]\n",
    "    return solver_settings\n",
    "\n",
    "def run_on_testset(data, testdata, **kwargs):\n",
    "    solutions = []\n",
    "    for point in testdata:\n",
    "        model = MINLPModel(data, min_volume=\"kwargs\", **kwargs)\n",
    "        solutions.append(model.find_min_f_hat(point, solver_settings=generate_solver_settings(kwargs)))\n",
    "    return solutions, model.solver_settings\n",
    "\n",
    "def run_on_whole_dataset(data):\n",
    "    return run_on_testset(data, data)\n",
    "\n",
    "def run_on_fixed_size_sample(data, size, **kwargs):\n",
    "    np.random.seed(0)\n",
    "    N = len(data)\n",
    "    assert size <= N, \"size larger than dataset\"\n",
    "    sample_indices = np.random.choice(N, max(1, size))\n",
    "    return run_on_testset(data, data[sample_indices], **kwargs)\n",
    "\n",
    "def run_on_fraction(data, fraction, **kwargs):\n",
    "    return run_on_fixed_size_sample(data, round(len(data)*fraction), **kwargs)\n",
    "\n",
    "def benchmark_and_store_result(expression: Callable):\n",
    "    start = time.time()\n",
    "    res = expression.__call__()\n",
    "    end = time.time()\n",
    "    return *res, end - start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n",
      "main MILP was unbounded. Resolving with arbitrary bound values of (-1e+15, 1e+15) on the objective. You can change this bound with the option obj_bound.\n",
      "infeasibility detected in deactivate_trivial_constraints\n",
      "Feasibility subproblem infeasible. This should never happen.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "for parameter_combo in itertools.product(*parameters):\n",
    "    for dataset in datasets.values():\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # dataset parameters\n",
    "            mlflow.log_param(\"size\", dataset.size)\n",
    "            mlflow.log_param(\"dimension\", dataset.dimension)\n",
    "            mlflow.log_param(\"density\", dataset.scale)\n",
    "            mlflow.log_param(\"source\", dataset.source)\n",
    "\n",
    "            # option parameters\n",
    "            for parameter in parameter_combo:\n",
    "                mlflow.log_param(parameter.name, parameter.value)\n",
    "\n",
    "            parameter_dict = {parameter.name : parameter.value for parameter in parameter_combo}\n",
    "\n",
    "            # timing results\n",
    "            f_hats, solver_settings, time_passed = benchmark_and_store_result(lambda: run_on_fixed_size_sample(dataset.data,1, **parameter_dict))\n",
    "            mlflow.log_metric(\"time\", time_passed)\n",
    "            mlflow.log_metric(\"average f_hat\", np.mean(f_hats))\n",
    "\n",
    "            for k,v in solver_settings.items():\n",
    "                mlflow.log_param(k,v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}