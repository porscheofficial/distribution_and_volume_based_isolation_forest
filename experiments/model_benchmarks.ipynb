{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarks to try and improve the run time of minlp solver scaling up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "np.random.seed(42)\n",
    "\n",
    "# For starters, we'll simply benchmark on fixed randomly generated test sets of increasing size, dimension and scale (decreased density).\n",
    "sizes = [100]\n",
    "dimensions = [2,3]\n",
    "scales = [1]\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", \"data size dimension scale source\")\n",
    "\n",
    "datasets = {(i,j,k): Dataset(np.random.rand(i,j) * k, i, j, k, \"random\") for i,j,k in itertools.product(sizes, dimensions, scales)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Below we're setting values for the min_volume and absolute errors that are based on the fact that we here know the underlying distribution to be uniform, which means that the true underlying $f(h)$ value for all patterns $h$ is actually $1$. We really only care about compuational effects, but the hope is that these are roughly the kinds of ranges that are relevant when dealing with actual anomaly detection problems and other distributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import pyomo\n",
    "from typing import Callable, List\n",
    "import time\n",
    "from rare_pattern_detect.minlp_based import MINLPModel\n",
    "\n",
    "Parameter = namedtuple(\"Parameter\", \"name value\")\n",
    "# solver parameters\n",
    "bound_included = [Parameter(\"bound included\", v) for v in [0.05, 1]]\n",
    "initial_patterns = [Parameter(\"initial pattern\", v) for v in [\"minimal\", \"maximal\"]]\n",
    "min_volumes = [Parameter(\"min volume\", v) for v in [0.05, 0.1]]\n",
    "absolute_errors = [Parameter(\"absolute error\", v) for v in [1e-3, 0.1]] # thinking this through I believe whatever absolute error we set, we can simply add it to the epsilon of the pac performance.\n",
    "relative_errors = [Parameter(\"relative error\", v) for v in [1e-3, 0.1]] # this becomes useful especially if we set epsilon to be relative to an estimate of f.\n",
    "# use_parallel_threads = [2, 4, 6]\n",
    "# use_different_solvers = [Falase, ]\n",
    "\n",
    "parameters = [bound_included,\n",
    "              initial_patterns,\n",
    "              min_volumes,\n",
    "              absolute_errors,\n",
    "              relative_errors\n",
    "              ]\n",
    "\n",
    "def generate_solver_settings(in_dict):\n",
    "    solver_settings = {\"tee\": False}\n",
    "    if \"absolute error\" in in_dict:\n",
    "        solver_settings[\"absolute_bound_tolerance\"] = in_dict[\"absolute error\"]\n",
    "    if \"relative error\" in in_dict:\n",
    "        solver_settings[\"relative_bound_tolerance\"] = in_dict[\"relative error\"]\n",
    "    return solver_settings\n",
    "\n",
    "def run_on_testset(data, testdata, **kwargs):\n",
    "    solutions = []\n",
    "    optimal_count = 0\n",
    "    for point in testdata:\n",
    "        model = MINLPModel(data, min_volume=\"kwargs\", **kwargs)\n",
    "        status, result = model.find_min_f_hat(point, solver_settings=generate_solver_settings(kwargs))\n",
    "        if status == \"ok\":\n",
    "            solutions.append(result)\n",
    "            optimal_count += 1\n",
    "    return solutions, optimal_count, model.solver_settings\n",
    "\n",
    "def run_on_whole_dataset(data):\n",
    "    return run_on_testset(data, data)\n",
    "\n",
    "def run_on_fixed_size_sample(data, size, **kwargs):\n",
    "    np.random.seed(0)\n",
    "    N = len(data)\n",
    "    assert size <= N, \"size larger than dataset\"\n",
    "    sample_indices = np.random.choice(N, max(1, size))\n",
    "    return run_on_testset(data, data[sample_indices], **kwargs)\n",
    "\n",
    "def run_on_fraction(data, fraction, **kwargs):\n",
    "    return run_on_fixed_size_sample(data, round(len(data)*fraction), **kwargs)\n",
    "\n",
    "def benchmark_and_store_result(expression: Callable):\n",
    "    start = time.time()\n",
    "    res = expression.__call__()\n",
    "    end = time.time()\n",
    "    return *res, end - start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# def run_experiment(title, datasets, parameters: [[Parameter]], execution_function)\n",
    "\n",
    "mlflow.set_experiment(\"MINLP Tweaking Small after debugging\")\n",
    "run_number = 0\n",
    "for parameter_combo in itertools.product(*parameters):\n",
    "    for dataset in datasets.values():\n",
    "        run_number += 1\n",
    "        print(f\"Running experiment number {run_number} for dataset {(dataset.size, dataset.dimension)} with parameters {[(p.name, p.value) for p in parameter_combo]}\")\n",
    "        with mlflow.start_run():\n",
    "\n",
    "            # dataset parameters\n",
    "            mlflow.log_param(\"size\", dataset.size)\n",
    "            mlflow.log_param(\"dimension\", dataset.dimension)\n",
    "            mlflow.log_param(\"density\", dataset.scale)\n",
    "            mlflow.log_param(\"source\", dataset.source)\n",
    "\n",
    "            # option parameters\n",
    "            for parameter in parameter_combo:\n",
    "                mlflow.log_param(parameter.name, parameter.value)\n",
    "\n",
    "            parameter_dict = {parameter.name : parameter.value for parameter in parameter_combo}\n",
    "\n",
    "            # min_volume is expressed as fraction of scale\n",
    "            parameter_dict[\"min volume\"] *= dataset.scale\n",
    "\n",
    "            # timing results\n",
    "            sample_size = 10\n",
    "            f_hats, optimal_count, solver_settings, time_passed = benchmark_and_store_result(lambda: run_on_fixed_size_sample(dataset.data,sample_size, **parameter_dict))\n",
    "            performance = np.mean(f_hats)\n",
    "            mlflow.log_metric(\"time\", time_passed)\n",
    "            mlflow.log_metric(\"optimal count\", optimal_count)\n",
    "            mlflow.log_metric(\"average f_hat\", performance)\n",
    "            print(f\"Results: average f_hat: {performance}, time: {time_passed}\")\n",
    "\n",
    "            for k,v in solver_settings.items():\n",
    "                mlflow.log_param(k,v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mlflow ui"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results for the isolation forest\n",
    "\n",
    "Here, instead of running the exact solver, we use the sampling procedure from the isolation forest, but we use family of scoring functions that are based on the sampled distribution of f_hat."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from rare_pattern_detect.if_based import IFBasedRarePatternDetect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "data = datasets[100,2,1].data\n",
    "\n",
    "if_pac_estimator = IFBasedRarePatternDetect(\n",
    "    n_estimators = 100\n",
    ")\n",
    "if_pac_estimator.fit(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([17.04063821, 25.63850336, 35.05091907, 33.63006719, 32.62918506,\n       68.70508772, 30.15832041, 34.87071737, 47.91774972, 34.80101429,\n       27.17125349, 34.22865241, 22.38836887, 24.32404293, 36.33376854,\n       53.66740099, 56.05771344, 62.56877749, 38.11368896, 24.63523048])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if_pac_estimator._get_pac_rpad_estimate(data[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}