{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pyomo.environ as pyo\n",
    "import pyomo.gdp as gdp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MIN_AREA = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import math \n",
    "\n",
    "class PatternSpaceType(Enum):\n",
    "    AXIS_ALIGNED_HYPER_RECTANGLES = 1\n",
    "    HALF_SPACES = 2\n",
    "\n",
    "class PatternSpace:\n",
    "    def __init__(self, type: PatternSpaceType, cutoff: float = 0):\n",
    "        self.type = type\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def calculate_coeff(self, **kwargs):\n",
    "        if self.type == PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES:\n",
    "            epsilon = kwargs[\"epsilon\"]\n",
    "            delta = kwargs[\"delta\"]\n",
    "            N = kwargs[\"N\"]\n",
    "            d = kwargs[\"d\"]\n",
    "            v = 2 * d\n",
    "            # TODO: FIll in details\n",
    "            # min_area = math.sqrt((1 / N) * (256 / epsilon**2) * ( v * math.log(256 / epsilon**2) + math.log(8 / delta)))\n",
    "            min_area = MIN_AREA\n",
    "            print(\"calculating cutoff dynamic min_area : \", min_area)\n",
    "            self.cutoff = min_area\n",
    "\n",
    "\n",
    "#from rare_pattern_detect.patterns import PatternSpace\n",
    "\n",
    "def contains(point: np.ndarray, largest_bounding_area) -> bool:\n",
    "    return all((largest_bounding_area[0,:] <= point.T) & (point.T <= largest_bounding_area[1,:]))\n",
    "\n",
    "def minlp_has_rare_pattern(\n",
    "    x, training_data, pattern_space: PatternSpace, mu, debugging_minlp_model=False\n",
    "):\n",
    "    min_area = MIN_AREA # pattern_space.cutoff\n",
    "    model = MINLPModel(training_data, min_area)\n",
    "\n",
    "    # Checking if point is included in the largest bounding area defined by the training set\n",
    "    if contains(x, model.largest_bounding_area):\n",
    "        solution = model.classify(x, debugging_minlp_model)\n",
    "\n",
    "        # Parse solution output\n",
    "        if solution is not None: \n",
    "            # If the minlp model was feasible and a solution was found \n",
    "            # then we return the model and the label that contains \n",
    "            # if the point is anomalous or not (bool)\n",
    "            res = (model, solution <= mu)\n",
    "        else:\n",
    "            # If for some reasons the model encountered an error \n",
    "            # while trying to solve the minlp model\n",
    "            print(\"Error when classifying a point: \", x, model.largest_bounding_area)\n",
    "            res = (None, None)\n",
    "    else:\n",
    "        print(\"point to be classified outside of the limits: anomaly\")\n",
    "        # no need to solve the minlp model for this point.\n",
    "        # Since the point lies outside of the largest point area, then it must be an anomaly (True)\n",
    "        res = (None, True)\n",
    "\n",
    "    return res\n",
    "        \n",
    "\n",
    "class MINLPModel:\n",
    "    def __init__(self, training_set: np.array, min_area: float):\n",
    "        assert min_area != 0.0, \"min_area is zero !! This should never happen -> f_hat is zero -> everything anomaleous\"\n",
    "        self.training_set = training_set  # a N x d matrix\n",
    "        self.min_area = min_area  # the smallest allowed area\n",
    "        self.N, self.d = self.training_set.shape\n",
    "        self.Nrange, self.drange = (range(x) for x in self.training_set.shape)\n",
    "        self.largest_bounding_area = np.array(\n",
    "            [\n",
    "                [np.min(self.training_set[:, 0]), np.min(self.training_set[:, 1])],\n",
    "                [np.max(self.training_set[:, 0]), np.max(self.training_set[:, 1])],\n",
    "            ]\n",
    "        )\n",
    "        self.model = self.create_model()\n",
    "        self.minimized_f_hats = np.zeros(self.N, float)\n",
    "\n",
    "    def create_model(self):\n",
    "        def _pattern_area():\n",
    "            return pyo.prod(model.interval_lengths[i] for i in self.drange)\n",
    "\n",
    "        # define model\n",
    "        model = pyo.ConcreteModel()\n",
    "\n",
    "        ## variables\n",
    "\n",
    "        # x is a 2d vector\n",
    "        model.pattern = pyo.Var(self.drange , self.drange) #  , bounds=adjust_largest_pattern_bounds)\n",
    "\n",
    "        # y is a boolean vector of size N\n",
    "        model.included = pyo.Var(self.Nrange, within=pyo.Binary, initialize=0)\n",
    "\n",
    "        # auxiliary variables\n",
    "        model.interval_lengths = pyo.Var(self.drange, within=pyo.NonNegativeReals)\n",
    "        model.point_left_of_pattern = pyo.Var(\n",
    "            self.Nrange, self.drange, within=pyo.Binary, initialize=0\n",
    "        )\n",
    "        model.point_right_of_pattern = pyo.Var(\n",
    "            self.Nrange, self.drange, within=pyo.Binary, initialize=0\n",
    "        )\n",
    "\n",
    "        ## objective (minimised by default)\n",
    "        model.obj = pyo.Objective(\n",
    "            expr=sum(model.included[i] for i in self.Nrange) / _pattern_area(),\n",
    "            sense=pyo.minimize,\n",
    "        )\n",
    "\n",
    "        ## constraints\n",
    "\n",
    "        # pattern area needs to exceed min_area\n",
    "        model.area_constraint = pyo.Constraint(expr=_pattern_area() >= self.min_area)\n",
    "\n",
    "        # training points included in model.included lie within the pattern (NB: In principle we would need to ensure that points not included are also\n",
    "        # not included in model.included. However, since including points outside the pattern increases the objective, this is covered.)\n",
    "\n",
    "        model.include_constraint = pyo.ConstraintList()\n",
    "        model.enforce_point_left_of_pattern = pyo.ConstraintList()\n",
    "        model.enforce_point_right_of_pattern = pyo.ConstraintList()\n",
    "        M = 100000\n",
    "        for j in self.Nrange:\n",
    "            for i in self.drange:\n",
    "                # enforcing auxiliary variables are correct: point_left_of_pattern[j,i] is True iff the jth training point lies strictly outside the pattern in ith dimension, etc.\n",
    "                model.enforce_point_left_of_pattern.add(\n",
    "                    (model.point_left_of_pattern[j, i] * M + self.training_set[j, i])\n",
    "                    >= model.pattern[0, i]\n",
    "                )\n",
    "                model.enforce_point_left_of_pattern.add(\n",
    "                    self.training_set[j, i] + 1e-3\n",
    "                    <= (\n",
    "                        model.pattern[0, i]\n",
    "                        + (1 - model.point_left_of_pattern[j, i]) * M\n",
    "                    )\n",
    "                )\n",
    "                model.enforce_point_right_of_pattern.add(\n",
    "                    self.training_set[j, i]\n",
    "                    <= (model.pattern[1, i] + model.point_right_of_pattern[j, i] * M)\n",
    "                )\n",
    "                model.enforce_point_right_of_pattern.add(\n",
    "                    (\n",
    "                        (1 - model.point_right_of_pattern[j, i]) * M\n",
    "                        + self.training_set[j, i]\n",
    "                    )\n",
    "                    >= (model.pattern[1, i] + 1e-3)\n",
    "                )\n",
    "\n",
    "            model.include_constraint.add(\n",
    "                # key bit: this constraint enforces that the model.included for jth point can be set to 0 only if the point is not contained in the pattern (as witnessed by the fact\n",
    "                # that the corresponding auxiliary variables are all 0)\n",
    "                model.included[j]\n",
    "                + sum(\n",
    "                    model.point_right_of_pattern[j, i]\n",
    "                    + model.point_left_of_pattern[j, i]\n",
    "                    for i in self.drange\n",
    "                )\n",
    "                >= 1\n",
    "            )\n",
    "\n",
    "        # connect auxiliary variables: interval lengths are differences of pattern points \n",
    "        # and set bounds of the pattern to be optmized\n",
    "        model.interval_constraint = pyo.ConstraintList()\n",
    "        model.pattern_constraint = pyo.ConstraintList()\n",
    "        for i in self.drange:\n",
    "            model.pattern_constraint.add(\n",
    "                model.pattern[0,i] >= np.min(self.training_set[:,i])\n",
    "            )\n",
    "            model.pattern_constraint.add(\n",
    "                model.pattern[1,i]  <= np.max(self.training_set[:,i])\n",
    "            )\n",
    "            model.interval_constraint.add(\n",
    "                model.interval_lengths[i] == model.pattern[1, i] - model.pattern[0, i]\n",
    "            )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def extract_points_included_in_pattern(self):  \n",
    "        return np.array([self.training_set[i] for i in self.model.included if np.round(self.model.included[i].value, 1) == 1.0])\n",
    "\n",
    "    def extract_pattern(self):\n",
    "        intervals = np.zeros((2, 2), dtype=float)\n",
    "        for _, j in enumerate(self.model.pattern):\n",
    "            intervals[j] = self.model.pattern[j].value\n",
    "        return intervals.T\n",
    "\n",
    "    def classify(self, point_to_be_classified: np.array, tee):\n",
    "        \"\"\"\n",
    "        This function evaluates one testing point and returns the calculated objective (f_hat)\n",
    "        \"\"\"\n",
    "        # point to be classified is a 1 x d array\n",
    "        self.add_point_to_model(point_to_be_classified)\n",
    "        _ = pyo.SolverFactory(\"mindtpy\").solve(\n",
    "            self.model,\n",
    "            strategy=\"OA\",\n",
    "            mip_solver=\"glpk\",\n",
    "            nlp_solver=\"ipopt\",\n",
    "            tee=tee,\n",
    "        )\n",
    "        try:\n",
    "            res = pyo.value(self.model.obj)\n",
    "        except: \n",
    "            print(\"-classify- Something went wrong with the solver: \", point_to_be_classified)\n",
    "            res = None \n",
    "        finally:\n",
    "            self.minimized_f_hats = np.round(res,2) if res is not None else None\n",
    "            return res\n",
    "         \n",
    "\n",
    "    def add_point_to_model(self, point):\n",
    "        # point to be classified lies in pattern\n",
    "        point = point.squeeze()\n",
    "        assert point.shape == (2,)\n",
    "        self.model.point_constraint = pyo.ConstraintList()\n",
    "        for i in self.drange:\n",
    "            # x[i] <= point[i] <= x[i + d], for all i\n",
    "            self.model.point_constraint.add(\n",
    "                self.model.pattern[0, i] <= point[i]\n",
    "            )\n",
    "            self.model.point_constraint.add(\n",
    "                point[i] <= self.model.pattern[1, i]\n",
    "            )\n",
    "\n",
    "\n",
    "#from rare_pattern_detect.patterns import PatternSpaceType\n",
    "#from rare_pattern_detect.minlp_based import minlp_has_rare_pattern\n",
    "\n",
    "import unittest\n",
    "class RarePatternDetect:\n",
    "    def __init__(self, delta, tau, epsilon, pattern_space: PatternSpace):\n",
    "        self.training_data = None\n",
    "        self.delta = delta\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.pattern_space = pattern_space\n",
    "\n",
    "        if pattern_space.type == PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES:\n",
    "            self.has_rare_pattern = minlp_has_rare_pattern\n",
    "        else:\n",
    "            self.has_rare_pattern = None\n",
    "\n",
    "    def load_training_data(self, training_data):\n",
    "        self.training_data = training_data\n",
    "        N, d = training_data.shape\n",
    "        self.pattern_space.cutoff = self.pattern_space.calculate_coeff(\n",
    "            epsilon=self.epsilon, delta=self.delta, N=N, d=d\n",
    "        )\n",
    "\n",
    "    def is_anomalous(self, x):\n",
    "        _, pred = self.has_rare_pattern(\n",
    "            x, \n",
    "            self.training_data, \n",
    "            self.pattern_space, \n",
    "            self.tau + self.epsilon / 2\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    ## Added for ADBench\n",
    "    def fit(self, X):\n",
    "        return self.load_training_data(X)\n",
    "\n",
    "    ## Added for ADBench\n",
    "    def predict_score(self, X_test):\n",
    "        return self.is_anomalous(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training_set =  np.array(\n",
    "#     [[0.0, 0.0], [2.0, 0.0], [0.0, 2.0], [2.0, 2.0]]\n",
    "# )\n",
    "training_set = multivariate_normal.rvs(size=(1000,2))\n",
    "# point_to_be_classified = np.array([[0.25,0.25]])\n",
    "# testing_set = point_to_be_classified\n",
    "testing_set = multivariate_normal.rvs(size=(10,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "def draw_largest_bounding_area(interval, ax):\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        xy=(interval[0,0], interval[1,0]),\n",
    "        width=interval[0,1] - interval[0,0],\n",
    "        height=interval[1,1] - interval[1,0],\n",
    "        facecolor='none',\n",
    "        edgecolor=cmap(0.5),\n",
    "        label= \"largest bounding area\",\n",
    "        linewidth=3.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "def draw2dpattern(interval, ax, classfication_result, minimized_f_hat, total):\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        xy=(interval[0,0], interval[1,0]),\n",
    "        width=interval[0,1] - interval[0,0],\n",
    "        height=interval[1,1] - interval[1,0],\n",
    "        facecolor='none',\n",
    "        edgecolor=cmap(0/total),\n",
    "        label=f\"point anomalous: {classfication_result} | f_hat: {minimized_f_hat}\"\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# min_area"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,7)\n",
    "\n",
    "points_outside_of_largest_bounding_area = 0\n",
    "indeces = []\n",
    "predictions = []\n",
    "\n",
    "mu, epsilon, delta = 0.1, 0.1, 0.1\n",
    "N, d = training_set.shape\n",
    "v = 2 * d \n",
    "#min_area = math.sqrt((1 / N) * (256 / epsilon**2) * ( v * math.log(256 / epsilon**2) + math.log(8 / delta))) # TODO: FIll in details\n",
    "min_area = MIN_AREA # 41 # 41\n",
    "print(\"min_area: \", min_area)\n",
    "lba = np.array(\n",
    "    [\n",
    "        [np.min(training_set[:, 0]), np.min(training_set[:, 1])],\n",
    "        [np.max(training_set[:, 0]), np.max(training_set[:, 1])],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# This is just a guess that delivers mixed \"acceptable\" results (predictions)\n",
    "#min_area = 2.0 # 0.1 # \n",
    "#min_area = np.prod(np.apply_along_axis(lambda i: i[1] - i[0], axis=0, arr=lba)) / np.sqrt(len(training_set))\n",
    "#assert min_area > 0, \"min area of largest bounding area is negative\"\n",
    "#print(\"min_area: \", min_area)\n",
    "\n",
    "print(N,d,v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for i, point_to_be_classified in enumerate(testing_set):\n",
    "    ## \n",
    "    print(f\"------- itr: {i} -------\")\n",
    "    print(f\" point_to_be_classified: {point_to_be_classified}\")\n",
    "\n",
    "    pattern_space =  PatternSpace(\n",
    "        type = PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES, \n",
    "        cutoff = min_area\n",
    "    )\n",
    "    \n",
    "    minlp_model, classification_result = minlp_has_rare_pattern(\n",
    "        point_to_be_classified,\n",
    "        training_set,\n",
    "        pattern_space,\n",
    "        mu,\n",
    "        debugging_minlp_model=False,\n",
    "    )\n",
    "    print(\"Classification result: \",classification_result)\n",
    "    predictions.append(classification_result)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    if minlp_model is not None:\n",
    "        result = (\n",
    "            # min_area,\n",
    "            mu,\n",
    "            classification_result,\n",
    "            minlp_model\n",
    "        )\n",
    "        included_points = minlp_model.extract_points_included_in_pattern()\n",
    "        calculated_pattern = minlp_model.extract_pattern()\n",
    "        area_calculated_pattern = np.prod(np.apply_along_axis(lambda i: i[1] - i[0], axis=1, arr=calculated_pattern))\n",
    "        print(\"area_calculated_pattern: \", area_calculated_pattern)\n",
    "        assert area_calculated_pattern > 0, \"area of minimized pattern is negative\"\n",
    "        largest_bounding_area = minlp_model.largest_bounding_area\n",
    "        [plt.scatter(\n",
    "            x=p[0], \n",
    "            y=p[1], \n",
    "            marker=\"o\", \n",
    "            s=200, \n",
    "            label=f\"included point: {np.argwhere(training_set == p)[0,0]}\") for _, p in enumerate(included_points)\n",
    "        ]\n",
    "        \n",
    "        plt.legend([f\"area of calculated pattern: {area_calculated_pattern}\"])\n",
    "        draw_largest_bounding_area(largest_bounding_area.T, ax)\n",
    "        draw2dpattern(calculated_pattern, ax, classification_result, minlp_model.minimized_f_hats, len(training_set))\n",
    "        plt.legend(bbox_to_anchor=(1.1, 1.1))\n",
    "    else:\n",
    "        print(f\"minlp model is none and classification results: {classification_result}\")\n",
    "        draw_largest_bounding_area(lba.T, ax)\n",
    "        ax.legend()\n",
    "        points_outside_of_largest_bounding_area += 1 \n",
    "        indeces.append(i)\n",
    "\n",
    "    plt.scatter(x=training_set[:,0], y=training_set[:,1])\n",
    "    plt.scatter(\n",
    "        x=point_to_be_classified[0], \n",
    "        y=point_to_be_classified[1], \n",
    "        marker=\"x\",  \n",
    "        s=200, \n",
    "        label=\"target\") \n",
    "    # ax.title()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"-- END __\")\n",
    "print(\"Points that lie outside of the largest bounding area defined by the training set: \",points_outside_of_largest_bounding_area) \n",
    "print(\"indices of the points: \", indeces)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing RarePatternDetect class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(training_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rpd = RarePatternDetect(\n",
    "    delta=0.1,\n",
    "    tau=0.1,\n",
    "    epsilon=0.1,\n",
    "    pattern_space = PatternSpace(\n",
    "        PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES, \n",
    "        cutoff = 0.41 # 3 #min_area\n",
    "    )\n",
    ")\n",
    "rpd.load_training_data(training_set)\n",
    "preds = [rpd.is_anomalous(point_to_be_classified) for _, point_to_be_classified in enumerate(testing_set)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -> should get the same results\n",
    "# @TODO: add assert preds in np.array([True, False]) and no \"None\"\n",
    "assert None not in preds \n",
    "assert None not in predictions\n",
    "# assert preds == predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds == predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment: comparing with Isolation Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "X = training_set\n",
    "clf = IsolationForest(random_state=0).fit(X)\n",
    "r = clf.predict(testing_set) # -1 anomalie\n",
    "r = np.where(r==1, r, False).astype(bool)\n",
    "print(\"r: \",r)\n",
    "count_similar_predictions = np.count_nonzero(r == preds)\n",
    "per_similar_predictions = count_similar_predictions/len(testing_set)\n",
    "count_similar_predictions,  per_similar_predictions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Notes: \n",
    "        # There is still an error that happens sometimes ... model should never return None as a prediction\n",
    "            # -> can be fixed by setting a very small min_area, which leads to all points being labeled as anomaleous\n",
    "        # why are all the points classified as anomalous? \n",
    "            #  -> f_hat is zero -> model able to find a pattern -> require bigger min_area?\n",
    "                # -> DONE: tried some stuff out to get mixed classification results\n",
    "                    # -> sqrt(number_of_training_points) give a mixed classification \n",
    "                        # (some patterns contain points from the training set. \n",
    "                        # Therefore, f_hat might be bigger. \n",
    "                        # Hence, the point is anomalous)\n",
    "\n",
    "    # -> honestly maybe a circle in 2D as a pattern might make more sense of this \n",
    "\n",
    "# @Done: transfer to python files and call the model through the RarePatternDetect(.ipynb -> .py) and refactor\n",
    "# @Done: wrapping up the tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the ADBench API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rpd = RarePatternDetect(\n",
    "    delta=0.1,\n",
    "    tau=0.1,\n",
    "    epsilon=0.1,\n",
    "    pattern_space = PatternSpace(\n",
    "        PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES, \n",
    "        cutoff = 4.1 # 3 #min_area\n",
    "    )\n",
    ")\n",
    "rpd.fit(training_set)\n",
    "preds = [rpd.predict_score(point_to_be_classified) for _, point_to_be_classified in enumerate(testing_set)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# -> should get the same results\n",
    "# @TODO: add assert preds in np.array([True, False]) and no \"None\"\n",
    "assert None not in preds \n",
    "assert None not in predictions\n",
    "# assert preds == predictions"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds == predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment: comparing with Isolation Forest"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "X = training_set\n",
    "clf = IsolationForest(random_state=0).fit(X)\n",
    "r = clf.predict(testing_set) # -1 anomalie\n",
    "r = np.where(r==1, r, False).astype(bool)\n",
    "print(\"r: \",r)\n",
    "count_similar_predictions = np.count_nonzero(r == preds)\n",
    "per_similar_predictions = count_similar_predictions/len(testing_set)\n",
    "count_similar_predictions,  per_similar_predictions\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "r:  [ True  True  True False  True]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 0.8)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Notes: \n",
    "        # There is still an error that happens sometimes ... model should never return None as a prediction\n",
    "            # -> can be fixed by setting a very small min_area, which leads to all points being labeled as anomaleous\n",
    "        # why are all the points classified as anomalous? \n",
    "            #  -> f_hat is zero -> model able to find a pattern -> require bigger min_area?\n",
    "                # -> DONE: tried some stuff out to get mixed classification results\n",
    "                    # -> sqrt(number_of_training_points) give a mixed classification \n",
    "                        # (some patterns contain points from the training set. \n",
    "                        # Therefore, f_hat might be bigger. \n",
    "                        # Hence, the point is anomalous)\n",
    "\n",
    "    # -> honestly maybe a circle in 2D as a pattern might make more sense of this \n",
    "\n",
    "# @Done: transfer to python files and call the model through the RarePatternDetect(.ipynb -> .py) and refactor\n",
    "# @Done: wrapping up the tests"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the ADBench API"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rpd = RarePatternDetect(\n",
    "    delta=0.1,\n",
    "    tau=0.1,\n",
    "    epsilon=0.1,\n",
    "    pattern_space = PatternSpace(\n",
    "        PatternSpaceType.AXIS_ALIGNED_HYPER_RECTANGLES, \n",
    "        cutoff = 4.1 # 3 #min_area\n",
    "    )\n",
    ")\n",
    "rpd.fit(training_set)\n",
    "preds = [rpd.predict_score(point_to_be_classified) for _, point_to_be_classified in enumerate(testing_set)]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "calculating cutoff dynamic min_area :  0.41\n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.15 64-bit ('pac-venv')"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "4ede7f29d864d30436995b20b34666eece4fa61a1cdb58253be3ba6b07adb827"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}